{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio Deep Retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QInk_g_jLaVf"
      },
      "source": [
        "!rm -r sample_data/"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYx38RqMSD4g"
      },
      "source": [
        "!pip install tf_slim\n",
        "!pip install scann"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwqDkpvmFIfL"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "from typing import Mapping\n",
        "from itertools import chain"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiE5UI8kR-kB"
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git\n",
        "%cd models/research/audioset/vggish\n",
        "!curl -O https://storage.googleapis.com/audioset/vggish_model.ckpt\n",
        "!curl -O https://storage.googleapis.com/audioset/vggish_pca_params.npz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brxugRDvBuc3"
      },
      "source": [
        "# Get labels and indexes of youtube noises\n",
        "!wget http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/balanced_train_segments.csv\n",
        "!wget http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/class_labels_indices.csv\n",
        "!wget storage.googleapis.com/us_audioset/youtube_corpus/v1/features/features.tar.gz\n",
        "\n",
        "# Extract dataset\n",
        "!tar -xf features.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF2Gjzn4FOK7"
      },
      "source": [
        "def load_labels_map() -> Mapping[int, str]:\n",
        "  # Build index to label map\n",
        "  index_label_map = dict()\n",
        "\n",
        "  class_labels_file = \"class_labels_indices.csv\"\n",
        "  with open(class_labels_file) as csv_file:\n",
        "    csvreader = csv.reader(csv_file)\n",
        "\n",
        "    # Skip header\n",
        "    fields = next(csvreader)\n",
        "\n",
        "    # extracting each data row one by one\n",
        "    for row in csvreader:\n",
        "      mid_label_map.update({int(row[0]) : row[2]})   # For example {\"8\" : \"Shout\"}\n",
        "\n",
        "  return mid_label_map"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSiQeKNWa0Td"
      },
      "source": [
        "!ls /content/audioset_v1_embeddings/bal_train/a1.tfrecord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPLUSOe6WyQs"
      },
      "source": [
        "# Load audio tfrecords\n",
        "#!ls /content/audioset_v1_embeddings/bal_train/\n",
        "for example_str in tf.python_io.tf_record_iterator(\"/content/audioset_v1_embeddings/bal_train/00.tfrecord\"):\n",
        "    seq_example = tf.train.SequenceExample.FromString(example_str)\n",
        "    print(seq_example.context.feature['video_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyJ_StxgczfI",
        "outputId": "f169ec18-193b-47d8-faa8-1dcdc3088f53"
      },
      "source": [
        "# Takes approximately 45 - 50 seconds to run\n",
        "# Load audio tfrecords\n",
        "#!ls /content/audioset_v1_embeddings/bal_train/\n",
        "files = os.listdir('/content/audioset_v1_embeddings/bal_train/')\n",
        "video_audio_map = dict()\n",
        "min_sec_found = 10 # doesnt matter\n",
        "\n",
        "for filename in files:\n",
        "  if not filename.endswith(\".tfrecord\"):\n",
        "    continue\n",
        "\n",
        "  for example_str in tf.compat.v1.io.tf_record_iterator(os.path.join(\"/content/audioset_v1_embeddings/bal_train/\", filename)):\n",
        "    seq_example = tf.train.SequenceExample.FromString(example_str)\n",
        "    min_sec_found = min(min_sec_found, len(seq_example.feature_lists.feature_list['audio_embedding'].feature))\n",
        "    if len(seq_example.feature_lists.feature_list['audio_embedding'].feature) >= 5:\n",
        "      bytes_2d_list = seq_example.feature_lists.feature_list['audio_embedding'].feature[0:5]\n",
        "      flattened_byte_list = []\n",
        "      for bytes_list in bytes_2d_list:\n",
        "        flattened_byte_list.extend(np.frombuffer(bytes_list.bytes_list.value[0], dtype=np.uint8))\n",
        "      video_audio_map.update({str(seq_example.context.feature['video_id'].bytes_list.value[0], 'utf-8'): flattened_byte_list})\n",
        "print(\"SMALLEST MIN_SEC_FOUND: \", min_sec_found)"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMALLEST MIN_SEC_FOUND:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kH6RoIjoCeX"
      },
      "source": [
        "dataset = np.empty((0, 640), np.uint8)\n",
        "index_video_map = dict()\n",
        "for idx, feature_list_key in enumerate(video_audio_map.keys()):\n",
        "  index_video_map.update({idx : feature_list_key})\n",
        "  feature_list = video_audio_map[feature_list_key]\n",
        "  dataset = np.append(dataset, np.array([feature_list]), axis=0)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiYQ-2tvWz5k"
      },
      "source": [
        "# Take approximately 42 seconds\n",
        "# Build ScaNN index\n",
        "import scann\n",
        "dataset = np.array(list(video_audio_map.values()))\n",
        "num_results = 10 \n",
        "searcher = scann.scann_ops_pybind.builder(dataset, num_results, \"dot_product\").tree(\n",
        "    num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(\n",
        "    2, anisotropic_quantization_threshold=0.2).reorder(100).build()"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYyHcbwEsUDH"
      },
      "source": [
        "queries = np.array(np.ones_like(640, shape=(1, 640)))\n",
        "neighbors, distances = searcher.search_batched(queries)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p6DxIKQs36y",
        "outputId": "0cf84c50-982f-4092-b02f-9c1e016197f3"
      },
      "source": [
        "for neighbor in neighbors[0]:\n",
        "  video_id = index_video_map[neighbor]\n",
        "\n",
        "  print(f\"http://youtube.com/watch?v={video_id}\")"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://youtube.com/watch?v=ArsKCV3rkc4\n",
            "http://youtube.com/watch?v=4HSkwF586ro\n",
            "http://youtube.com/watch?v=QM4qxOYDwHo\n",
            "http://youtube.com/watch?v=ZaeARmx4m0k\n",
            "http://youtube.com/watch?v=DRGpwij9No8\n",
            "http://youtube.com/watch?v=UGtYWC-ddF4\n",
            "http://youtube.com/watch?v=zFRreJxXDFw\n",
            "http://youtube.com/watch?v=Dj6vz-bsHXY\n",
            "http://youtube.com/watch?v=smTo8842-5c\n",
            "http://youtube.com/watch?v=FOxIDRWTHZc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb5lG6yRT_N4"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "import vggish_input\n",
        "import vggish_params\n",
        "import vggish_postprocess\n",
        "import vggish_slim\n",
        "\n",
        "# Paths to downloaded VGGish files.\n",
        "checkpoint_path = 'vggish_model.ckpt'\n",
        "pca_params_path = 'vggish_pca_params.npz'\n",
        "\n",
        "# Relative tolerance of errors in mean and standard deviation of embeddings.\n",
        "rel_error = 0.1  # Up to 10%\n",
        "\n",
        "# Generate a 1 kHz sine wave at 44.1 kHz (we use a high sampling rate\n",
        "# to test resampling to 16 kHz during feature extraction).\n",
        "########## REPLACE WITH CODE TO LOAD WAVEFORM FROM USER\n",
        "num_secs = 4\n",
        "freq = 1000\n",
        "sr = 44100\n",
        "t = np.arange(0, num_secs, 1 / sr)\n",
        "x = np.sin(2 * np.pi * freq * t)\n",
        "\n",
        "# Produce a batch of log mel spectrogram examples.\n",
        "input_batch = vggish_input.waveform_to_examples(x, sr)\n",
        "np.testing.assert_equal(\n",
        "    input_batch.shape,\n",
        "    [num_secs, vggish_params.NUM_FRAMES, vggish_params.NUM_BANDS])\n",
        "\n",
        "# Define VGGish, load the checkpoint, and run the batch through the model to\n",
        "# produce embeddings.\n",
        "with tf.Graph().as_default(), tf.Session() as sess:\n",
        "  vggish_slim.define_vggish_slim()\n",
        "  vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
        "\n",
        "  features_tensor = sess.graph.get_tensor_by_name(\n",
        "      vggish_params.INPUT_TENSOR_NAME)\n",
        "  embedding_tensor = sess.graph.get_tensor_by_name(\n",
        "      vggish_params.OUTPUT_TENSOR_NAME)\n",
        "  [embedding_batch] = sess.run([embedding_tensor],\n",
        "                               feed_dict={features_tensor: input_batch})\n",
        "  print('Num of embeddings: ', len(embedding_batch))\n",
        "  print('VGGish embedding: ', embedding_batch[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}