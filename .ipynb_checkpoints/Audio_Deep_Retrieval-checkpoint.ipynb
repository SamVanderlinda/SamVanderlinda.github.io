{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QInk_g_jLaVf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rm -r sample_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RYx38RqMSD4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf_slim\n",
      "  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in c:\\users\\buime\\anaconda3\\lib\\site-packages (from tf_slim) (0.11.0)\n",
      "Requirement already satisfied: six in c:\\users\\buime\\anaconda3\\lib\\site-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n",
      "Installing collected packages: tf-slim\n",
      "Successfully installed tf-slim-1.1.0\n",
      "Collecting scann\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Could not find a version that satisfies the requirement scann (from versions: none)\n",
      "ERROR: No matching distribution found for scann\n"
     ]
    }
   ],
   "source": [
    "!pip install tf_slim\n",
    "!pip install scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WwqDkpvmFIfL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from typing import Mapping\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiE5UI8kR-kB"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/tensorflow/models.git\n",
    "%cd models/research/audioset/vggish\n",
    "!curl -O https://storage.googleapis.com/audioset/vggish_model.ckpt\n",
    "!curl -O https://storage.googleapis.com/audioset/vggish_pca_params.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brxugRDvBuc3"
   },
   "outputs": [],
   "source": [
    "# Get labels and indexes of youtube noises\n",
    "!wget http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/balanced_train_segments.csv\n",
    "!wget http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/class_labels_indices.csv\n",
    "!wget storage.googleapis.com/us_audioset/youtube_corpus/v1/features/features.tar.gz\n",
    "\n",
    "# Extract dataset\n",
    "!tar -xf features.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qF2Gjzn4FOK7"
   },
   "outputs": [],
   "source": [
    "def load_labels_map() -> Mapping[int, str]:\n",
    "  # Build index to label map\n",
    "  index_label_map = dict()\n",
    "\n",
    "  class_labels_file = \"class_labels_indices.csv\"\n",
    "  with open(class_labels_file) as csv_file:\n",
    "    csvreader = csv.reader(csv_file)\n",
    "\n",
    "    # Skip header\n",
    "    fields = next(csvreader)\n",
    "\n",
    "    # extracting each data row one by one\n",
    "    for row in csvreader:\n",
    "      mid_label_map.update({int(row[0]) : row[2]})   # For example {\"8\" : \"Shout\"}\n",
    "\n",
    "  return mid_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSiQeKNWa0Td"
   },
   "outputs": [],
   "source": [
    "!ls /content/audioset_v1_embeddings/bal_train/a1.tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPLUSOe6WyQs"
   },
   "outputs": [],
   "source": [
    "# Load audio tfrecords\n",
    "#!ls /content/audioset_v1_embeddings/bal_train/\n",
    "for example_str in tf.python_io.tf_record_iterator(\"/content/audioset_v1_embeddings/bal_train/00.tfrecord\"):\n",
    "    seq_example = tf.train.SequenceExample.FromString(example_str)\n",
    "    print(seq_example.context.feature['video_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyJ_StxgczfI",
    "outputId": "f169ec18-193b-47d8-faa8-1dcdc3088f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMALLEST MIN_SEC_FOUND:  1\n"
     ]
    }
   ],
   "source": [
    "# Takes approximately 45 - 50 seconds to run\n",
    "# Load audio tfrecords\n",
    "#!ls /content/audioset_v1_embeddings/bal_train/\n",
    "files = os.listdir('/content/audioset_v1_embeddings/bal_train/')\n",
    "video_audio_map = dict()\n",
    "min_sec_found = 10 # doesnt matter\n",
    "\n",
    "for filename in files:\n",
    "  if not filename.endswith(\".tfrecord\"):\n",
    "    continue\n",
    "\n",
    "  for example_str in tf.compat.v1.io.tf_record_iterator(os.path.join(\"/content/audioset_v1_embeddings/bal_train/\", filename)):\n",
    "    seq_example = tf.train.SequenceExample.FromString(example_str)\n",
    "    min_sec_found = min(min_sec_found, len(seq_example.feature_lists.feature_list['audio_embedding'].feature))\n",
    "    if len(seq_example.feature_lists.feature_list['audio_embedding'].feature) >= 5:\n",
    "      bytes_2d_list = seq_example.feature_lists.feature_list['audio_embedding'].feature[0:5]\n",
    "      flattened_byte_list = []\n",
    "      for bytes_list in bytes_2d_list:\n",
    "        flattened_byte_list.extend(np.frombuffer(bytes_list.bytes_list.value[0], dtype=np.uint8))\n",
    "      video_audio_map.update({str(seq_example.context.feature['video_id'].bytes_list.value[0], 'utf-8'): flattened_byte_list})\n",
    "print(\"SMALLEST MIN_SEC_FOUND: \", min_sec_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "0kH6RoIjoCeX"
   },
   "outputs": [],
   "source": [
    "dataset = np.empty((0, 640), np.uint8)\n",
    "index_video_map = dict()\n",
    "for idx, feature_list_key in enumerate(video_audio_map.keys()):\n",
    "  index_video_map.update({idx : feature_list_key})\n",
    "  feature_list = video_audio_map[feature_list_key]\n",
    "  dataset = np.append(dataset, np.array([feature_list]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "WiYQ-2tvWz5k"
   },
   "outputs": [],
   "source": [
    "# Take approximately 42 seconds\n",
    "# Build ScaNN index\n",
    "import scann\n",
    "dataset = np.array(list(video_audio_map.values()))\n",
    "num_results = 10 \n",
    "searcher = scann.scann_ops_pybind.builder(dataset, num_results, \"dot_product\").tree(\n",
    "    num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(\n",
    "    2, anisotropic_quantization_threshold=0.2).reorder(100).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "dYyHcbwEsUDH"
   },
   "outputs": [],
   "source": [
    "queries = np.array(np.ones_like(640, shape=(1, 640)))\n",
    "neighbors, distances = searcher.search_batched(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2p6DxIKQs36y",
    "outputId": "0cf84c50-982f-4092-b02f-9c1e016197f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://youtube.com/watch?v=ArsKCV3rkc4\n",
      "http://youtube.com/watch?v=4HSkwF586ro\n",
      "http://youtube.com/watch?v=QM4qxOYDwHo\n",
      "http://youtube.com/watch?v=ZaeARmx4m0k\n",
      "http://youtube.com/watch?v=DRGpwij9No8\n",
      "http://youtube.com/watch?v=UGtYWC-ddF4\n",
      "http://youtube.com/watch?v=zFRreJxXDFw\n",
      "http://youtube.com/watch?v=Dj6vz-bsHXY\n",
      "http://youtube.com/watch?v=smTo8842-5c\n",
      "http://youtube.com/watch?v=FOxIDRWTHZc\n"
     ]
    }
   ],
   "source": [
    "for neighbor in neighbors[0]:\n",
    "  video_id = index_video_map[neighbor]\n",
    "\n",
    "  print(f\"http://youtube.com/watch?v={video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lb5lG6yRT_N4"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_postprocess\n",
    "import vggish_slim\n",
    "\n",
    "# Paths to downloaded VGGish files.\n",
    "checkpoint_path = 'vggish_model.ckpt'\n",
    "pca_params_path = 'vggish_pca_params.npz'\n",
    "\n",
    "# Relative tolerance of errors in mean and standard deviation of embeddings.\n",
    "rel_error = 0.1  # Up to 10%\n",
    "\n",
    "# Generate a 1 kHz sine wave at 44.1 kHz (we use a high sampling rate\n",
    "# to test resampling to 16 kHz during feature extraction).\n",
    "########## REPLACE WITH CODE TO LOAD WAVEFORM FROM USER\n",
    "num_secs = 4\n",
    "freq = 1000\n",
    "sr = 44100\n",
    "t = np.arange(0, num_secs, 1 / sr)\n",
    "x = np.sin(2 * np.pi * freq * t)\n",
    "\n",
    "# Produce a batch of log mel spectrogram examples.\n",
    "input_batch = vggish_input.waveform_to_examples(x, sr)\n",
    "np.testing.assert_equal(\n",
    "    input_batch.shape,\n",
    "    [num_secs, vggish_params.NUM_FRAMES, vggish_params.NUM_BANDS])\n",
    "\n",
    "# Define VGGish, load the checkpoint, and run the batch through the model to\n",
    "# produce embeddings.\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "  vggish_slim.define_vggish_slim()\n",
    "  vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
    "\n",
    "  features_tensor = sess.graph.get_tensor_by_name(\n",
    "      vggish_params.INPUT_TENSOR_NAME)\n",
    "  embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "      vggish_params.OUTPUT_TENSOR_NAME)\n",
    "  [embedding_batch] = sess.run([embedding_tensor],\n",
    "                               feed_dict={features_tensor: input_batch})\n",
    "  print('Num of embeddings: ', len(embedding_batch))\n",
    "  print('VGGish embedding: ', embedding_batch[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Audio Deep Retrieval.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
